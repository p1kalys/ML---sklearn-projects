{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis of Tweets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMuXDS8Mcq4g80LZrFZgt9K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/p1kalys/ML---sklearn-projects/blob/main/Sentiment_Analysis_of_Tweets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fL0c9Bn7EyCb",
        "outputId": "80294542-d63a-4aa3-ae1f-636797269fcd"
      },
      "source": [
        "!pip install vaderSentiment"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting vaderSentiment\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/fc/310e16254683c1ed35eeb97386986d6c00bc29df17ce280aed64d55537e9/vaderSentiment-3.3.2-py2.py3-none-any.whl (125kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 15.4MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 20kB 21.4MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30kB 12.2MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 40kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 61kB 5.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 71kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 81kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 92kB 6.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 102kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 112kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 122kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vaderSentiment) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2.10)\n",
            "Installing collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWCpsVl9HdZH",
        "outputId": "f884a6f4-8374-4333-c222-60d750667b92"
      },
      "source": [
        "!pip install contractions"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/93/f4/0ec4a458e4368cc3be2c799411ecf0bc961930e566dadb9624563821b3a6/contractions-0.0.52-py2.py3-none-any.whl\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading https://files.pythonhosted.org/packages/d3/fe/021d7d76961b5ceb9f8d022c4138461d83beff36c3938dc424586085e559/textsearch-0.0.21-py2.py3-none-any.whl\n",
            "Collecting anyascii\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/14/666cd44bf53f36a961544af592cb5c5c800013f9c51a4745af8d7c17362a/anyascii-0.2.0-py3-none-any.whl (283kB)\n",
            "\u001b[K     |████████████████████████████████| 286kB 5.3MB/s \n",
            "\u001b[?25hCollecting pyahocorasick\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7f/c2/eae730037ae1cbbfaa229d27030d1d5e34a1e41114b21447d1202ae9c220/pyahocorasick-1.4.2.tar.gz (321kB)\n",
            "\u001b[K     |████████████████████████████████| 327kB 33.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85405 sha256=073e507bde8732cf865990b731f4a23381bb63e6b8da59d4b8ce70a04da9b2a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/03/34/77e3ece0bba8b86bfac88a79f923b36d805cad63caeba38842\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: anyascii, pyahocorasick, textsearch, contractions\n",
            "Successfully installed anyascii-0.2.0 contractions-0.0.52 pyahocorasick-1.4.2 textsearch-0.0.21\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkbW9M06G3a2"
      },
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "vs = SentimentIntensityAnalyzer()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpeBCNOVaXwa"
      },
      "source": [
        "import sys"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBBox5ogG5pz"
      },
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import nltk\n",
        "import contractions"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggC0F9ofHuG9",
        "outputId": "fdf3b6ee-a922-42fe-9bf7-6507b2ccb171"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VBfx32dH4hr",
        "outputId": "658f5fed-b278-48b0-d560-387d557b0d1c"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCQrigNeHZYn"
      },
      "source": [
        "def con(text):\n",
        "  expand = contractions.fix(text)\n",
        "  return expand \n",
        "\n",
        "import re\n",
        "def remove_sp(text):\n",
        "  pattern = r'[^A-Za-z0-9\\s]'\n",
        "  text = re.sub(pattern,'',text)\n",
        "  return text\n",
        "\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "tokenizer  = ToktokTokenizer()\n",
        "stopword_list = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  tokens = tokenizer.tokenize(text)\n",
        "  tokens = [token.strip() for token in tokens]\n",
        "  filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "  filtered_text = ' '.join(filtered_tokens)\n",
        "  return filtered_text\n",
        "\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "# lemmatize string\n",
        "def lemmatize_word(text):\n",
        "  word_tokens = word_tokenize(text)\n",
        "  w = [lemmatizer.lemmatize(word, pos='v') for word in word_tokens]\n",
        "  lemmas = ' '.join(w) \n",
        "  return lemmas\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vect = TfidfVectorizer(stop_words=\"english\")"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "ZBnutDkqItCH",
        "outputId": "f1a59a73-522c-4fd5-e9fb-d6e4d43fe63b"
      },
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/p1kalys/ML---sklearn-projects/main/Tweets.csv')\n",
        "df.head()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>31963</td>\n",
              "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>31964</td>\n",
              "      <td>@user #white #supremacists want everyone to s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>31965</td>\n",
              "      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>31966</td>\n",
              "      <td>is the hp and the cursed child book up for res...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>31967</td>\n",
              "      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id                                              tweet\n",
              "0  31963  #studiolife #aislife #requires #passion #dedic...\n",
              "1  31964   @user #white #supremacists want everyone to s...\n",
              "2  31965  safe ways to heal your #acne!!    #altwaystohe...\n",
              "3  31966  is the hp and the cursed child book up for res...\n",
              "4  31967    3rd #bihday to my amazing, hilarious #nephew..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRn1sglfJqhb"
      },
      "source": [
        "df.tweet = df.tweet.apply(lambda x:x.lower())\n",
        "df.tweet = df.tweet.apply(con)\n",
        "df.tweet = df.tweet.apply(remove_sp)\n",
        "df.tweet = df.tweet.apply(remove_stopwords)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "x6CpafkXKYpP",
        "outputId": "31ba85ce-8809-4fcc-aaab-8055386507cf"
      },
      "source": [
        "df['compound'] = df['tweet'].apply(lambda x:vs.polarity_scores(x)['compound'])\n",
        "\n",
        "df"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>compound</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>31963</td>\n",
              "      <td>studiolife aislife requires passion dedication...</td>\n",
              "      <td>0.4588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>31964</td>\n",
              "      <td>user white supremacists want everyone see new ...</td>\n",
              "      <td>-0.1779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>31965</td>\n",
              "      <td>safe ways heal acne altwaystoheal healthy healing</td>\n",
              "      <td>0.6808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>31966</td>\n",
              "      <td>hp cursed child book reservations already yes ...</td>\n",
              "      <td>0.6908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>31967</td>\n",
              "      <td>3rd bihday amazing hilarious nephew eli ahmir ...</td>\n",
              "      <td>0.8519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17192</th>\n",
              "      <td>49155</td>\n",
              "      <td>thought factory leftright polarisation trump u...</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17193</th>\n",
              "      <td>49156</td>\n",
              "      <td>feeling like mermaid hairflip neverready forma...</td>\n",
              "      <td>0.4588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17194</th>\n",
              "      <td>49157</td>\n",
              "      <td>hillary campaigned today ohioomg amp used word...</td>\n",
              "      <td>0.3612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17195</th>\n",
              "      <td>49158</td>\n",
              "      <td>happy work conference right mindset leads cult...</td>\n",
              "      <td>0.5719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17196</th>\n",
              "      <td>49159</td>\n",
              "      <td>song glad free download shoegaze newmusic newsong</td>\n",
              "      <td>0.7430</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>17197 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          id                                              tweet  compound\n",
              "0      31963  studiolife aislife requires passion dedication...    0.4588\n",
              "1      31964  user white supremacists want everyone see new ...   -0.1779\n",
              "2      31965  safe ways heal acne altwaystoheal healthy healing    0.6808\n",
              "3      31966  hp cursed child book reservations already yes ...    0.6908\n",
              "4      31967  3rd bihday amazing hilarious nephew eli ahmir ...    0.8519\n",
              "...      ...                                                ...       ...\n",
              "17192  49155  thought factory leftright polarisation trump u...    0.0000\n",
              "17193  49156  feeling like mermaid hairflip neverready forma...    0.4588\n",
              "17194  49157  hillary campaigned today ohioomg amp used word...    0.3612\n",
              "17195  49158  happy work conference right mindset leads cult...    0.5719\n",
              "17196  49159  song glad free download shoegaze newmusic newsong    0.7430\n",
              "\n",
              "[17197 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "q4I2qFzvKucK",
        "outputId": "4017974e-83eb-4cff-e80f-6903ef22a122"
      },
      "source": [
        "df.tweet = df.tweet.apply(lemmatize_word)\n",
        "df.head()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>compound</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>31963</td>\n",
              "      <td>studiolife aislife require passion dedication ...</td>\n",
              "      <td>0.4588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>31964</td>\n",
              "      <td>user white supremacists want everyone see new ...</td>\n",
              "      <td>-0.1779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>31965</td>\n",
              "      <td>safe ways heal acne altwaystoheal healthy heal</td>\n",
              "      <td>0.6808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>31966</td>\n",
              "      <td>hp curse child book reservations already yes h...</td>\n",
              "      <td>0.6908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>31967</td>\n",
              "      <td>3rd bihday amaze hilarious nephew eli ahmir un...</td>\n",
              "      <td>0.8519</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id                                              tweet  compound\n",
              "0  31963  studiolife aislife require passion dedication ...    0.4588\n",
              "1  31964  user white supremacists want everyone see new ...   -0.1779\n",
              "2  31965     safe ways heal acne altwaystoheal healthy heal    0.6808\n",
              "3  31966  hp curse child book reservations already yes h...    0.6908\n",
              "4  31967  3rd bihday amaze hilarious nephew eli ahmir un...    0.8519"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txUUP7-ILNl0",
        "outputId": "b3c755d8-804c-4d6b-912c-adbeee6f5c9f"
      },
      "source": [
        "for i in range(len(df['compound'])):\n",
        "  if df.compound[i]<=-0.05:\n",
        "    df.compound[i] = \"negative\"\n",
        "  elif df.compound[i]>=0.05:\n",
        "    df.compound[i] = \"positive\"\n",
        "  elif df.compound[i]>-0.05 and df.compound[i]<0.05:\n",
        "    df.compound[i] = \"neutral\""
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  iloc._setitem_with_indexer(indexer, value)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "gaLSwqyuLTy1",
        "outputId": "251bd53d-b200-4c6a-8e24-8af385f096dd"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>compound</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>31963</td>\n",
              "      <td>studiolife aislife require passion dedication ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>31964</td>\n",
              "      <td>user white supremacists want everyone see new ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>31965</td>\n",
              "      <td>safe ways heal acne altwaystoheal healthy heal</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>31966</td>\n",
              "      <td>hp curse child book reservations already yes h...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>31967</td>\n",
              "      <td>3rd bihday amaze hilarious nephew eli ahmir un...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id                                              tweet  compound\n",
              "0  31963  studiolife aislife require passion dedication ...  positive\n",
              "1  31964  user white supremacists want everyone see new ...  negative\n",
              "2  31965     safe ways heal acne altwaystoheal healthy heal  positive\n",
              "3  31966  hp curse child book reservations already yes h...  positive\n",
              "4  31967  3rd bihday amaze hilarious nephew eli ahmir un...  positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccYpXGPKLXib",
        "outputId": "4893cc40-2605-474a-ce41-e14db8b4a08a"
      },
      "source": [
        "df['compound'].value_counts()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "positive    9018\n",
              "neutral     4810\n",
              "negative    3369\n",
              "Name: compound, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj7ZEaAELiFl"
      },
      "source": [
        "x = df['tweet'].values\n",
        "y = df['compound'].values"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7IpeylpLlzf",
        "outputId": "544f9472-eec0-4dbb-e786-a2aaa398bde3"
      },
      "source": [
        "x.reshape(-1,1)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['studiolife aislife require passion dedication willpower find newmaterials'],\n",
              "       ['user white supremacists want everyone see new bird movie heres'],\n",
              "       ['safe ways heal acne altwaystoheal healthy heal'],\n",
              "       ...,\n",
              "       ['hillary campaign today ohioomg amp use word like assetsampliability never clinton say theeword radicalization'],\n",
              "       ['happy work conference right mindset lead cultureofdevelopment organizations work mindset'],\n",
              "       ['song glad free download shoegaze newmusic newsong']],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aslOeDb1LsOS",
        "outputId": "d98166f4-4be6-4d35-b0a2-f1f8e1431f8d"
      },
      "source": [
        "y"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['positive', 'negative', 'positive', ..., 'positive', 'positive',\n",
              "       'positive'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJevfdj_LvLg"
      },
      "source": [
        "# Split the data\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,random_state=0)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f11fmbkXLzYx",
        "outputId": "d00cef50-fcf0-49c4-f7e5-6c991d393d76"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(12897,)\n",
            "(4300,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNre8gvdL3KZ",
        "outputId": "b91a9724-c657-4196-bdd6-35187a625b01"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "model = make_pipeline(TfidfVectorizer(),SVC())\n",
        "\n",
        "model.fit(x_train,y_train)\n",
        "\n",
        "y_pred = model.predict(x_test)\n",
        "y_pred"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['positive', 'positive', 'negative', ..., 'positive', 'positive',\n",
              "       'neutral'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6Dx2oJmL5v3",
        "outputId": "aba3d39c-f996-4e2d-a4fb-c88aec2ad0da"
      },
      "source": [
        "from sklearn.metrics import accuracy_score,confusion_matrix\n",
        "accuracy_score(y_pred,y_test)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8404651162790697"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0tRlH8OME1U",
        "outputId": "62f40504-10e4-4583-bc98-517b6349e1d5"
      },
      "source": [
        "confusion_matrix(y_pred,y_test)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 544,   26,   36],\n",
              "       [ 193, 1033,  180],\n",
              "       [ 130,  121, 2037]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kt0ECCUmMlQy",
        "outputId": "52ed1e4e-df45-4dd2-82e6-431d5a5b6190"
      },
      "source": [
        "text = input('Enter the text')\n",
        "text = text.lower()\n",
        "text = con(text)\n",
        "text = remove_sp(text)\n",
        "text = remove_stopwords(text)\n",
        "text = lemmatize_word(text)\n",
        "y_pred = model.predict([text])\n",
        "print(f'Predicted Analysis is {y_pred} ')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the text8 ia m busy right now call to anothher number 897654445\n",
            "Predicted Analysis is ['positive'] \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yaHUj0PM913",
        "outputId": "c74d7670-9a7d-4a82-ec6e-2b9dc9b086ab"
      },
      "source": [
        "import joblib\n",
        "joblib.dump(model,'Sentiment_Analysis')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Sentiment_Analysis']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJh2aeP4NL_g",
        "outputId": "d0ca744d-63ee-4e80-adb7-59c920d370ef"
      },
      "source": [
        "!pip install streamlit --quiet\n",
        "!pip install pyngrok==4.1.1\n",
        "from pyngrok import ngrok"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 7.8MB 4.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 174kB 45.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 112kB 51.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 10.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.2MB 43.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 9.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 122kB 59.8MB/s \n",
            "\u001b[?25h  Building wheel for blinker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: ipykernel 6.0.1 has requirement ipython>=7.23.1, but you'll have ipython 5.5.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement ipykernel~=4.10, but you'll have ipykernel 6.0.1 which is incompatible.\u001b[0m\n",
            "Collecting pyngrok==4.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/e4/a9/de2e15c92eb3aa4a2646ce3a7542317eb69ac47f667578ce8bf916320847/pyngrok-4.1.1.tar.gz\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyngrok==4.1.1) (0.16.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyngrok==4.1.1) (3.13)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-4.1.1-cp37-none-any.whl size=15985 sha256=f8773adf8159c67b7e550a04577e35811870621cee587aa6cef933453ee3d852\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/71/0d/1695f7c8815c0beb3b5d9b35d6eec9243c87e6070fbe3977fa\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-4.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pyh-9_bINOxX",
        "outputId": "9d65bf8c-7185-44f7-d543-dd219427fd70"
      },
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import nltk\n",
        "st.title('Sentiment Analysis')\n",
        "model = joblib.load('Sentiment_Analysis')\n",
        "import contractions\n",
        "def con(text):\n",
        "  expand = contractions.fix(text)\n",
        "  return expand \n",
        "\n",
        "import re\n",
        "def remove_sp(text):\n",
        "  pattern = r'[^A-Za-z0-9\\s]'\n",
        "  text = re.sub(pattern,'',text)\n",
        "  return text\n",
        "\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "tokenizer  = ToktokTokenizer()\n",
        "stopword_list = nltk.corpus.stopwords.words('english')\n",
        "def remove_stopwords(text):\n",
        "  tokens = tokenizer.tokenize(text)\n",
        "  tokens = [token.strip() for token in tokens]\n",
        "  filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "  filtered_text = ' '.join(filtered_tokens)\n",
        "  return filtered_text\n",
        "\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "# lemmatize string\n",
        "def lemmatize_word(text):\n",
        "  word_tokens = word_tokenize(text)\n",
        "  w = [lemmatizer.lemmatize(word, pos='v') for word in word_tokens]\n",
        "  lemmas = ' '.join(w) \n",
        "  return lemmas\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vect = TfidfVectorizer(stop_words=\"english\")\n",
        "\n",
        "text= st.text_input('Enter your message')\n",
        "\n",
        "if st.button('Predict'):\n",
        "  st.write('Result....')\n",
        "  text = text.lower()\n",
        "  text = con(text)\n",
        "  text = remove_sp(text)\n",
        "  text = remove_stopwords(text)\n",
        "  text = lemmatize_word(text)\n",
        "  y_pred = model.predict([text])\n",
        "  st.write(f'Predicted Analysis is {y_pred} ')\n",
        "  "
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "HWVLg8IqNR1B",
        "outputId": "ee06a1d9-e70e-4a6c-c925-ecfe0e00c234"
      },
      "source": [
        "!nohup streamlit run app.py &\n",
        "url = ngrok.connect(port='8501')\n",
        "url"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nohup: appending output to 'nohup.out'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'http://b20310f97fb8.ngrok.io'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpJOFTOYNT4Q"
      },
      "source": [
        ""
      ],
      "execution_count": 43,
      "outputs": []
    }
  ]
}